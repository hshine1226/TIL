(window.webpackJsonp=window.webpackJsonp||[]).push([[118],{477:function(t,s,a){"use strict";a.r(s);var n=a(25),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"beautifulsoup으로-크롤링-해보기"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#beautifulsoup으로-크롤링-해보기"}},[t._v("#")]),t._v(" BeautifulSoup으로 크롤링 해보기")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.crummy.com/software/BeautifulSoup/bs4/doc/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Beautiful Soup Documentation"),a("OutboundLink")],1),t._v("에서 문서를 참고할 수 있다.")]),t._v(" "),a("h2",{attrs:{id:"설치법"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#설치법"}},[t._v("#")]),t._v(" 설치법")]),t._v(" "),a("h3",{attrs:{id:"패키지-매니저를-통해서-설치하기"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#패키지-매니저를-통해서-설치하기"}},[t._v("#")]),t._v(" 패키지 매니저를 통해서 설치하기")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[t._v("apt-get")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" python3-bs4\n")])])]),a("h2",{attrs:{id:"사용법"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#사용법"}},[t._v("#")]),t._v(" 사용법")]),t._v(" "),a("p",[t._v("일단 사용하기 전에 BeautifulSoup을 import 해준다.")]),t._v(" "),a("p",[t._v("그리고 html 파일이 필요한데, 우리는 웹 페이지의 정보를 크롤릴 해볼 것이기 때문에 requests를 이용해서 html 정보를 받아올 것이다.")]),t._v(" "),a("p",[t._v("따라서 "),a("code",[t._v("requests")]),t._v("역시 import 해준다.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" requests\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" bs4 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BeautifulSoup\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# requests.get(url).content을 통해서 request를 통해 html을 받아올 수 잇다.")]),t._v("\nos"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("system"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'clear'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nurl "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://www.iban.com/currency-codes"')]),t._v("\nr "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" requests"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nhtml_doc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" r"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 그리고 BeautifulSoup을 통해서 해당 html을 받아올 수 있다.")]),t._v("\nsoup "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BeautifulSoup"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("html_doc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'html.parser'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"find-find-all"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#find-find-all"}},[t._v("#")]),t._v(" find(), find_all()")]),t._v(" "),a("p",[t._v("find_all() 메소드는 모든 문서의 결과들을 스캔한다.")]),t._v(" "),a("p",[t._v("만약 문서의 모든 title 태그를 스캔하고 싶다면 다음과 같이 스캔할 수 있다.")]),t._v(" "),a("p",[a("code",[t._v("find_all('title')")])]),t._v(" "),a("p",[t._v("find_all()이 모든 문서의 결과들을 스캔한다면, find()는 문서의 하나의 결과만 스캔한다.")]),t._v(" "),a("h2",{attrs:{id:"string"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#string"}},[t._v("#")]),t._v(" .string")]),t._v(" "),a("p",[t._v("만약 어떠한 태그에 "),a("code",[t._v(".string")]),t._v("을 달아준다면 그 태그가 가지고 있는 스트링만을 반환한다.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("markup "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'")]),t._v("\nsoup "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BeautifulSoup"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("markup"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntag "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" soup"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tag"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# I linked to example.com")]),t._v("\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);